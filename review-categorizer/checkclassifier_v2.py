# -*- coding: utf-8 -*-
"""Copie de catégorisation v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mHjUmCvtdHw5ozdwspZ7GETls8JxiSoB
"""

# !pip install spacy
# !python -m spacy download fr_core_news_sm
# !python -m spacy download es_core_news_sm
# !python -m spacy download en_core_web_sm
# !pip install transformers
# !pip install sentencepiece

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import spacy
import requests
import json
import traceback
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

###################################################
# Ségmentation de texte utilisant la librairie spacy qui consiste à découper
# le texte lorsqu'on croire les adverbes, poctuations et conjonctions #
###################################################


def english_segmentation(text):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)

    sentence = ""
    word_count = 0

    result = []

    excepted_token = ["AND", "ET", "THAT"]
    included_token = ["|", "."]

    for token in doc:
        # print(token.text, " => ", token.pos_)

        if (token.pos_ in ["CCONJ", "ADV"] and token.text.upper() not in excepted_token) or (token.text in included_token):

            if sentence != "":
                if (token.text == "," or token.text == "|") and word_count > 3:
                    result.append(sentence)
                    sentence = ""
                    word_count = 0
                    # print("Condition 1")
                elif token.text == "," and word_count <= 3:
                    sentence = "".join([sentence, token.text])
                    word_count += 1
                    # print("Condition 2")
                else:
                    result.append(sentence)
                    sentence = ""
                    word_count = 1
                    # print("Condition 3")
            else:
                sentence = "".join([sentence, token.text])
                word_count += 1
                # print("Condition 4")

        else:
            sentence = " ".join([sentence, token.text])
            word_count += 1
            # print("Condition 5")
            # print("=> ", sentence)

    if sentence != "" and len(sentence.split()) > 1:
        # print(sentence)
        result.append(sentence.strip())

    return [sentence.strip() for sentence in result if sentence.strip()]


def spanish_segmentation(text):
    nlp = spacy.load("es_core_web_sm")
    doc = nlp(text)

    sentence = ""
    word_count = 0

    result = []

    for token in doc:
        # print(token.text, " => ", token.pos_)

        if token.pos_ in ["CCONJ", "ADV"] or token.text == "|":

            if sentence != "":
                if (token.text == "," or token.text == "|") and word_count > 3:
                    result.append(sentence)
                    sentence = ""
                    word_count = 0
                    # print("Condition 1")
                elif token.text == "," and word_count <= 3:
                    sentence = "".join([sentence, token.text])
                    word_count += 1
                    # print("Condition 2")
                else:
                    result.append(sentence)
                    sentence = token.text
                    word_count = 1
                    # print("Condition 3")
            else:
                sentence = "".join([sentence, token.text])
                word_count += 1
                # print("Condition 4")

        else:
            sentence = " ".join([sentence, token.text])
            word_count += 1
            # print("Condition 5")
            # print("=> ", sentence)

    if sentence != "" and len(sentence.split()) > 1:
        # print(sentence)
        result.append(sentence.strip())

    return [sentence.strip() for sentence in result if sentence.strip()]


def french_segmentation(text):
    return [sentence.strip() for sentence in text.split('.') if sentence.strip()]


def segment_text(text, language):

    if language == "en":
        return english_segmentation(text)

    elif language == "fr":
        return french_segmentation(text)
    elif language == "es":
        return spanish_segmentation(text)
    else:
        return []


def classify_text(categories, text):
    # Initialize the zero-shot classification pipeline with the specified model
    # The device is set to -1 to use the CPU

    classifier = pipeline('zero-shot-classification',
                          device=-1, model='cross-encoder/nli-deberta-v3-base')

    # classifier = pipeline("zero-shot-classification", device=-1, model="facebook/bart-large-mnli")

    # Classify the text using the provided categories
    # The multi_labels parameter is set to True to allow multiple categories for a single text
    result = classifier(text, categories, multi_labels=True)
    # print(result)

    # Return the result of the classification
    return result


def analyse_text(review_item, labels, entity='review', min_score=0.8, language='fr'):
    def filter_labels(prediction, min_score=0.8):
        filtered_labels = []
        for label, score in zip(prediction['labels'], prediction['scores']):
            # print(label, score)
            if score >= min_score:
                filtered_labels.append((label, score))

        return filtered_labels

    def max_label(prediction):
        max_item = None

        if len(prediction['labels']):
            max_item = (prediction['labels'][0], prediction['scores'][0])
            for label, score in zip(prediction['labels'], prediction['scores']):
                if score > max_item[1]:
                    max_item = (label, score)

        return max_item

    def format_results(results, review, entity, categories):
        def format_line(line, review, entity, categories):
            obj = {
                "section": line[0],
                "category": line[1][0],
                "confidence": line[1][1],
                "checked": categories
            }

            if entity == "review":
                obj["review"] = str(review["id"])
                obj["socialPost"] = ""
                obj["socialComment"] = ""

            elif entity == "socialPost":
                obj["review"] = ""
                obj["socialPost"] = str(review["id"])
                obj["socialComment"] = ""

            elif entity == "socialComment":
                obj["review"] = ""
                obj["socialPost"] = ""
                obj["socialComment"] = str(review["id"])

            return obj

        return list(map(lambda x: format_line(x, review, entity, categories), results))

    def sentence_analyse(sentence, entity='review'):
        # print("\n\"============ Sentence ===========\"\n")
        words = sentence.split()
        # print("\n\"", sentence, "\"\n")

        current_sentence = ""
        last_sentence = ""

        last_category = None

        results = []

        for word in words:
            current_sentence += word + " "

            classifications = classify_text(labels, current_sentence.strip())

            categories = filter_labels(classifications, min_score)

            # print("\n################ Start ####################")
            # print("# Categorization result: ", categories)
            # print("# Last category: ", last_category)
            # print("# Last sentence: ", last_sentence)
            # print("# Current sentence: ", current_sentence)

            if last_category is None and len(categories) > 0:
                last_category = categories[0]
                # print("# Condition 1")
                # results.append((current_sentence.strip(), last_category))
            elif len(categories) > 0 and categories[0][0] != last_category[0]:
                # print("# Condition 2")
                # print("# =====> Ajout dans resultats")
                results.append((last_sentence.strip(), last_category))
                # print("# ", results)
                last_category = categories[0]
                current_sentence = word + " "
            else:
                # print("# Condition 3")
                last_sentence = current_sentence

            last_sentence = current_sentence

            # print("################## End ###################\n")

        # print("# ", last_sentence, " and ", last_category, " => ", last_sentence != "" and last_category)

        if last_sentence != "" and last_category:
            # print("# =====> Ajout dans resultats")
            results.append((current_sentence.strip(), last_category))
            # print("# result: ", results)

        if len(results) == 0:
            max_category = max_label(classifications)
            if max_category:
                results.append((current_sentence.strip(), max_category))

        return results

    results = []

    if review_item['text'] != "" and len(review_item['text']) > 25:

        sentences = segment_text(review_item["text"], language)

        # print("\n######################\n")
        # print("text => ", text)
        print(sentences)

        # results.extend(sentence_analyse(text))

        for sentence in sentences:
            classifications = sentence_analyse(sentence)
            results.extend(format_results(classifications,
                           review_item, entity, categories=labels))

    return results


def get_data_from_api(url, bearer_token, params=None):
    """
    Fetches data from an API with a bearer token and optional parameters.

    Args:
      url: The API endpoint URL.
      bearer_token: The bearer token for authentication.
      params: A dictionary of query parameters.

    Returns:
      The response object from the API call.
    """
    headers = {
        "Authorization": f"Bearer {bearer_token}"
    }

    print("Preparing request ...")

    response = None

    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()  # Raise an exception if the request was unsuccessful

    except requests.exceptions.RequestException as err:
        print("An error occurred:")
        print(err)
        print(traceback.format_exc())

    print("Request sent !")
    return response


def post_data_to_api(url, bearer_token, data):
    headers = {
        "Authorization": f"Bearer {bearer_token}"
    }

    response = None

    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        response.raise_for_status()  # Raise an exception if the request was unsuccessful
        # print(response.text)
    except requests.exceptions.RequestException as err:
        print("An error occurred:")
        print(traceback.format_exc())

    return response


def post_classifications(datas):
    print("\n Datas: ", datas, '\n')
    print("Uploading classifications ...")

    url = f"https://api-dev.nexties.fr/api/classification/upload"
    bearer_token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpYXQiOjE3MTY4OTAxNjAsImV4cCI6MzI3MjA5MDE2MCwicm9sZXMiOlsiUk9MRV9FUkVQIiwiUk9MRV9DVVNUT01FUiIsIlJPTEVfUEFSVE5FUiIsIlJPTEVfVVNFUiJdLCJ1c2VybmFtZSI6ImxhdXJlbnppb3NhbWJhbnlAZ21haWwuY29tIn0.AfCjRneev40fepe1YuPdI22BoHQznaYkAGyee19dxiYhBWR8YAr15HbhL4ewNyyoglazQTVaW4HmI5AmUIKG7L8_PhzJkso0F4o6W5Tpf8T-xvhv_eB1STM39DEmFx3DPDDxABdbm7Xxc1BE5trSBQ_mIz1d-Rcebkej7Rg4SCGHL0Wv6GJlhFH3RVB87y49ETQDWCK4icU4UKlo-q6CW2HzAEct3cXk6vWhU93sw8y_iqFmlvCKU_cMdb1BTqsqohZYQ1Nt2k8xNRiNBcl4yazWrI-2qJS33e9IUkjib5jWfnsvMuA8BzntozZk2ieD-K5MfavT1nyuRij5BI018w"

    response = post_data_to_api(url, bearer_token, data={
                                "data_content": datas})

    if response.status_code == 200:
        data = response.json()
        print(data)
        print("Uploaded successfully !")
        return data

    else:
        print(f"Error: {response.status_code}")
        return None

# Example usage:


def get_reviews(page=1, limit=10):

    tag = "652d515ba431b"  # 28-50
    # tag = "645de52f135e8" # Chalets du berger
    # tag = "66a21d3f105b8" # Hotel Lux Grand Gaube
    url = f"https://api-dev.nexties.fr/api/establishment/{tag}/reviews_to_classify"
    bearer_token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpYXQiOjE3MTY4OTAxNjAsImV4cCI6MzI3MjA5MDE2MCwicm9sZXMiOlsiUk9MRV9FUkVQIiwiUk9MRV9DVVNUT01FUiIsIlJPTEVfUEFSVE5FUiIsIlJPTEVfVVNFUiJdLCJ1c2VybmFtZSI6ImxhdXJlbnppb3NhbWJhbnlAZ21haWwuY29tIn0.AfCjRneev40fepe1YuPdI22BoHQznaYkAGyee19dxiYhBWR8YAr15HbhL4ewNyyoglazQTVaW4HmI5AmUIKG7L8_PhzJkso0F4o6W5Tpf8T-xvhv_eB1STM39DEmFx3DPDDxABdbm7Xxc1BE5trSBQ_mIz1d-Rcebkej7Rg4SCGHL0Wv6GJlhFH3RVB87y49ETQDWCK4icU4UKlo-q6CW2HzAEct3cXk6vWhU93sw8y_iqFmlvCKU_cMdb1BTqsqohZYQ1Nt2k8xNRiNBcl4yazWrI-2qJS33e9IUkjib5jWfnsvMuA8BzntozZk2ieD-K5MfavT1nyuRij5BI018w"
    params = {"all": "yes", "type": "reviews", "page": page, 'limit': limit}

    response = get_data_from_api(url, bearer_token, params)

    if response.status_code == 200:
        data = response.json()
        print(data)
        return data

    else:
        print(f"Error: {response.status_code}")
        return None


def get_all_reviews():
    results = []

    labels = ["Fourniture", "Food", "Location",
              "Welcome", "Confort", "Service"]

    page = 1
    while True:
        print("\n====== Récupération page ", page, " ======\n")
        data = get_reviews(page=page)

        # print(data)
        if data:
            for review in data['reviews']:
                # results = analyse_text(text, labels, language='fr')
                # print("\n############ final results:#############\nresult = ", results, "\n")

                # print("\n############ TEXTES ANGLAIS #############\n")

                # for text in texts2:
                if review['language'] and review['language'] == 'en':
                    print("\ntext => ", review['text'], ':\n')
                    result = analyse_text(
                        review, labels, entity='review', min_score=0.7, language='en')
                    print("\nresult => ", result, "\n")
                    results.extend(result)

            if len(results):
                post_classifications(results)

            results = []

            page += 1
        else:
            break

get_all_reviews()

# tab = [{"section": "Had a great meal with friends celebrating a birthday", "category": "Food", "confidence": 0.9611726999282837, "checked": ["Fourniture", "Food", "Location", "Welcome", "Confort", "Service"], "review": "47018", "socialPost": "", "socialComment": ""}, {"section": "Food , service was", "category": "Food", "confidence": 0.9846404194831848, "checked": ["Fourniture", "Food", "Location", "Welcome", "Confort", "Service"], "review": "47018", "socialPost": "", "socialComment": ""}]
# new_tab = json.dumps(tab)
# print(type(tab))
# print(type(new_tab))


def get_score(classifier, text):
    try:
        return classifier(text.replace('\"', "\'"))
    except Exception as e:
        print(e)
        return False


def compute_sentiment(text):
    model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    classifier = pipeline(
        'sentiment-analysis', model=model, tokenizer=tokenizer, device=-1)

    score_data = get_score(classifier, text)

    if score_data:
        confidence = score_data[0]['score']
        score_label = score_data[0]['label']

        score_stars = int(score_label.split()[0])
        feeling = "negative" if score_stars < 3 else (
            "positive" if score_stars > 3 else "neutre")

        return {'confidence': confidence, 'feeling': feeling}

    return False


def post_sentiments(datas):
    print("\n Datas: ", datas, '\n')
    print("Uploading sentiments ...")

    url = f"https://api-dev.nexties.fr/api/classification/feeling/categorization"
    bearer_token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpYXQiOjE3MTY4OTAxNjAsImV4cCI6MzI3MjA5MDE2MCwicm9sZXMiOlsiUk9MRV9FUkVQIiwiUk9MRV9DVVNUT01FUiIsIlJPTEVfUEFSVE5FUiIsIlJPTEVfVVNFUiJdLCJ1c2VybmFtZSI6ImxhdXJlbnppb3NhbWJhbnlAZ21haWwuY29tIn0.AfCjRneev40fepe1YuPdI22BoHQznaYkAGyee19dxiYhBWR8YAr15HbhL4ewNyyoglazQTVaW4HmI5AmUIKG7L8_PhzJkso0F4o6W5Tpf8T-xvhv_eB1STM39DEmFx3DPDDxABdbm7Xxc1BE5trSBQ_mIz1d-Rcebkej7Rg4SCGHL0Wv6GJlhFH3RVB87y49ETQDWCK4icU4UKlo-q6CW2HzAEct3cXk6vWhU93sw8y_iqFmlvCKU_cMdb1BTqsqohZYQ1Nt2k8xNRiNBcl4yazWrI-2qJS33e9IUkjib5jWfnsvMuA8BzntozZk2ieD-K5MfavT1nyuRij5BI018w"

    response = post_data_to_api(url, bearer_token, data={
                                'data_content': datas})

    if response.status_code == 200:
        data = response.json()
        print(data)
        print("Uploaded successfully !")
        return data

    else:
        print(f"Error: {response.status_code}")
        return None

# Example usage:


def get_sections(page=1, limit=10):

    tag = "652d515ba431b"  # 28-50
    # tag = "645de52f135e8" # Chalets du berger
    # tag = "66a21d3f105b8" # Hotel Lux Grand Gaube
    url = f"https://api-dev.nexties.fr/api/customer/classifications"
    bearer_token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpYXQiOjE3MTY4OTAxNjAsImV4cCI6MzI3MjA5MDE2MCwicm9sZXMiOlsiUk9MRV9FUkVQIiwiUk9MRV9DVVNUT01FUiIsIlJPTEVfUEFSVE5FUiIsIlJPTEVfVVNFUiJdLCJ1c2VybmFtZSI6ImxhdXJlbnppb3NhbWJhbnlAZ21haWwuY29tIn0.AfCjRneev40fepe1YuPdI22BoHQznaYkAGyee19dxiYhBWR8YAr15HbhL4ewNyyoglazQTVaW4HmI5AmUIKG7L8_PhzJkso0F4o6W5Tpf8T-xvhv_eB1STM39DEmFx3DPDDxABdbm7Xxc1BE5trSBQ_mIz1d-Rcebkej7Rg4SCGHL0Wv6GJlhFH3RVB87y49ETQDWCK4icU4UKlo-q6CW2HzAEct3cXk6vWhU93sw8y_iqFmlvCKU_cMdb1BTqsqohZYQ1Nt2k8xNRiNBcl4yazWrI-2qJS33e9IUkjib5jWfnsvMuA8BzntozZk2ieD-K5MfavT1nyuRij5BI018w"
    params = {"page": page, 'limit': limit}

    response = get_data_from_api(url, bearer_token, params)

    if response.status_code == 200:
        data = response.json()
        # print(data)
        return data

    else:
        print(f"Error: {response.status_code}")
        return None


def get_all_sections():
    results = []

    page = 1

    while True:
        print("\n====== Récupération page ", page, " ======\n")
        data = get_sections(page=page)

        print(data)
        if data and data['last_pages'] >= page:
            for line in data['items']:
                if line['section'] and len(line['section'].split()) > 1:
                    score = compute_sentiment(line['section'])
                    if score:
                        results.append({
                            'id': line['id'],
                            'feeling': score['feeling'],
                            'confidence_feeling': str(score['confidence'])
                        })

            if len(results):
                post_sentiments(results)

            results = []

            page += 1
        else:
            break

# get_all_sections()

# import socket

# hostname = socket.gethostname()
# ip_address = socket.gethostbyname(hostname)

# print(f"The IP address of this machine is {ip_address}")
